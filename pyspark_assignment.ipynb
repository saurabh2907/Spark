{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "hqM0ScSSpmCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! java -version"
      ],
      "metadata": {
        "id": "fZeu_reKRiRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "V2XxpZ72qJZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xf spark-3.0.1-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "XlZ9RgXxqROK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://downloads.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop2.tgz\n"
      ],
      "metadata": {
        "id": "lxuyI1X5qUgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf spark-3.3.1-bin-hadoop2.tgz"
      ],
      "metadata": {
        "id": "CebB_ZpCrE5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXwB62Gmo9ZX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop2\""
      ],
      "metadata": {
        "id": "IC40qGZDOOlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "9KK4lg8urlKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "-YSMu_Qvs_pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "vhXLIG_gtaYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import*\n",
        "\n"
      ],
      "metadata": {
        "id": "7jOUfKI703lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "connectionString=   ('mongodb+srv://saurabhpaliwal:emohGbcHXiceW25X@kafkademo.wtr499c.mongodb.net/?retryWrites=true&w=majority')\n",
        "spark = SparkSession\\\n",
        ".builder\\\n",
        ".master('local')\\\n",
        "    .config('spark.mongodb.input.uri',connectionString)\\\n",
        "    .config('spark.mongodb.output.uri', connectionString)\\\n",
        "    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.3.1')\\\n",
        ".getOrCreate()\n",
        " "
      ],
      "metadata": {
        "id": "ZWcXLftSOk_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading from MongoDB\n",
        "df_pro1 = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", connectionString).option(\"inferSchema\",True).option(\"database\", \"test\").option(\"collection\", \"time_province\").load()\n",
        "df_case1 = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", connectionString).option(\"inferSchema\",True).option(\"database\", \"test\").option(\"collection\", \"cases\").load()\n",
        "df_region1 = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\", connectionString).option(\"inferSchema\",True).option(\"database\", \"test\").option(\"collection\", \"region\").load()\n"
      ],
      "metadata": {
        "id": "1Cq8X68E7i0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pro2 = df_pro1.withColumn(\"_id\",col(\"_id\").cast(StringType()))  #changing data type\n",
        "df_case2 = df_case1.withColumn(\"_id\",col(\"_id\").cast(StringType())) \n",
        "df_region2 = df_region1.withColumn(\"_id\",col(\"_id\").cast(StringType())) "
      ],
      "metadata": {
        "id": "MtuP0yZZVpod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_pro2.select(\"_id\").show(truncate=False)"
      ],
      "metadata": {
        "id": "Z4hkhdTXb2XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pro3=df_pro2.withColumn(\"_id\", expr(\"substring(_id, 2,length(_id)-2)\")) # stripping {} froms the id column \n",
        "df_case3=df_case2.withColumn(\"_id\", expr(\"substring(_id, 2,length(_id)-2)\"))\n",
        "df_region3=df_region2.withColumn(\"_id\", expr(\"substring(_id, 2,length(_id)-2)\"))"
      ],
      "metadata": {
        "id": "J-3T0oW6Z6yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pro3.select('_id').show(truncate=False)"
      ],
      "metadata": {
        "id": "9wFGf_xHZD5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pro3.dtypes"
      ],
      "metadata": {
        "id": "ErP2eUf3hvNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# a. Read the data, show it and Count the number of records."
      ],
      "metadata": {
        "id": "MhK9LXUL_u6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_case3.show(truncate=False)\n",
        "print(\"the reords count is \", df_case.count())"
      ],
      "metadata": {
        "id": "sqkwJMHr7uVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_region3.show(truncate=False)\n",
        "print(\"the reords count is \", df_region.count())"
      ],
      "metadata": {
        "id": "1-ggK736-MNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pro3.show(truncate=False)\n",
        "print(\"the reords count is \", df_pro.count())"
      ],
      "metadata": {
        "id": "NhfpIFii-RLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# b. Describe the data with a describe function."
      ],
      "metadata": {
        "id": "GZHWVgNNAWiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_case3.describe().show()"
      ],
      "metadata": {
        "id": "225cCY7c-aqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pro3.describe().show()"
      ],
      "metadata": {
        "id": "ELyWiTnT-m9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_region3.describe().show()"
      ],
      "metadata": {
        "id": "Wsfa7WnOApcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# c. If there is any duplicate value drop it."
      ],
      "metadata": {
        "id": "j2AL39iEA0o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region_no_dups=df_region3.dropDuplicates()\n",
        "povince_no_dups=df_pro3.dropDuplicates()\n",
        "case_no_dups=df_case3.dropDuplicates()\n"
      ],
      "metadata": {
        "id": "LFKQ41WaAsoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# d. Use limit function for showcasing a limited number of records."
      ],
      "metadata": {
        "id": "FQvwo4q0Fsb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region_no_dups.limit(3).show()"
      ],
      "metadata": {
        "id": "EtqBBpD2BIWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# e. If you find the column name is not suitable, change the column name.[optional]"
      ],
      "metadata": {
        "id": "MpEffAReWcrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region_column_changed= region_no_dups.withColumnRenamed('province','state')"
      ],
      "metadata": {
        "id": "2_ZCeqinDkTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region_column_changed.show(6)"
      ],
      "metadata": {
        "id": "BnR_z6WGXYfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# f. Select the subset of the columns."
      ],
      "metadata": {
        "id": "TNvpbsmcXoYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region_column_changed.select('code','state','city', 'latitude','longitude').show(5)"
      ],
      "metadata": {
        "id": "esFZHg7AXZ6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# g. If there is any null value, fill it with any random value or drop it"
      ],
      "metadata": {
        "id": "FOftRI7bcfky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region_column_changed.dtypes"
      ],
      "metadata": {
        "id": "BTulT-DrTUQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region_column_changed.select([count(when(isnan(c), c)).alias(c) for c in region_column_changed.columns]).show() #no null values"
      ],
      "metadata": {
        "id": "8Nkzi2phYTYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "case_no_dups.select([count(when(isnan(c), c)).alias(c) for c in df_case.columns]).show()"
      ],
      "metadata": {
        "id": "UJQjPUVFct5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "case_no_dups.createOrReplaceTempView(\"cast\") #creating sql view\n",
        "df=spark.sql(\"select string(group) as Group_type_converted,* from  cast\") #changing column type \n",
        "df.dtypes "
      ],
      "metadata": {
        "id": "KpllkV_RgwPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "case_dropped=df.drop('group') #dropping pervious group column \n",
        "case_dropped.show(5)"
      ],
      "metadata": {
        "id": "dp3KhzfQqhmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "case_dropped.select([count(when(isnan(c), c)).alias(c) for c in case_dropped.columns]).show() #checking for counts for null vlaues "
      ],
      "metadata": {
        "id": "oivPlv4ihv7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "povince_no_dups.select([count(when(isnan(c), c)).alias(c) for c in povince_no_dups.columns]).show()  #checking for counts for null vlaues"
      ],
      "metadata": {
        "id": "0Cbh8-mrpvJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# h. Filter the data based on different columns or variables and perform the best analysis."
      ],
      "metadata": {
        "id": "HpThSLyau9Tk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) Which infection_case has highest confirmed cases among all the cases"
      ],
      "metadata": {
        "id": "dvMb36rpxv4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "case_no_dups.createOrReplaceTempView(\"case_no_dups\") #creating sql view\n",
        "df=spark.sql(\"select sum(confirmed) as highest_cases,infection_case from  case_no_dups group by infection_case order by highest_cases desc limit 1\") \n",
        "df.show()"
      ],
      "metadata": {
        "id": "1b_3U9IC2vuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Which province has highest deceased cases among all the province"
      ],
      "metadata": {
        "id": "0RZPzLipXv-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "povince_no_dups.createOrReplaceTempView(\"province_no_dups\") #creating sql view\n",
        "df1=spark.sql(\"select sum(deceased) as highest_deceased,province from  province_no_dups group by province order by highest_deceased desc limit 1\") \n",
        "df1.show()"
      ],
      "metadata": {
        "id": "QbEWF4ibe1f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# i. Sort the number of confirmed cases. Confirmed column is there in the dataset. Check with descending sort also."
      ],
      "metadata": {
        "id": "GsUPwiVb3e_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "case_no_dups.sort('confirmed').show()"
      ],
      "metadata": {
        "id": "I-Dg-r9d3uXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "case_no_dups.orderBy(col('confirmed').desc()).show()"
      ],
      "metadata": {
        "id": "aGE8jqvD6S0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# j. In case of any wrong data type, cast that data type from \n",
        "# integer to string or string to integer."
      ],
      "metadata": {
        "id": "-FRnMb6i7Evz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#solved at second cell right after reading data fro  mango db"
      ],
      "metadata": {
        "id": "wf9mwXdg-MQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k. Use group by on top of province and city column and agg it\n",
        "# with sum of confirmed cases. "
      ],
      "metadata": {
        "id": "w1Wef13W_Q7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "case_no_dups.groupBy(\"province\",\"city\").agg(sum('confirmed').alias('total_cases')).sort(desc(\"total_cases\")).show()"
      ],
      "metadata": {
        "id": "dZX7RIxG-Xrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# l. For joins we will need one more file.you can use region file.\n",
        "# User different different join methods."
      ],
      "metadata": {
        "id": "dBPjymoYHcNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "case_no_dups.show(3)"
      ],
      "metadata": {
        "id": "jJthyfKvG59k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "province_no_dups.show(3)"
      ],
      "metadata": {
        "id": "63-HY59lIUWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "region_column_changed.show(8)"
      ],
      "metadata": {
        "id": "DBL0g70VIXru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. what is total_count of university and confirmed per each province"
      ],
      "metadata": {
        "id": "NMkLe0wkTGMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_join =region_column_changed.join(case_no_dups, region_column_changed.state == case_no_dups.province,'inner')\n",
        "df_join.groupBy(\"state\").agg(sum('university_count').alias('total_university_count'),sum('confirmed').alias('total_confirmed')).show()"
      ],
      "metadata": {
        "id": "6wItv9ZzLKxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.what is total_count of nursing_home_count and confirmed per each province and city"
      ],
      "metadata": {
        "id": "uOvjdlPidE9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_join1 =region_column_changed.join(case_no_dups, (region_column_changed.state == case_no_dups.province) & \\\n",
        "(region_column_changed.city == case_no_dups.city),'left').select(region_column_changed.state,case_no_dups.city,case_no_dups.confirmed,region_column_changed.nursing_home_count)\n",
        "\n",
        "df_join1.groupBy('state','city').agg(sum('confirmed').alias('total_confirmed'),sum('nursing_home_count').alias('total_nursing_home_count')).show()\n"
      ],
      "metadata": {
        "id": "H9smMGBcQRZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. If you want, you can also use SQL with data frames. Let us try to\n",
        "# run some SQL on the cases table."
      ],
      "metadata": {
        "id": "XTXjsl7qdp8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "region_column_changed.createOrReplaceTempView(\"region\") #creating sql view\n",
        "case_no_dups.createOrReplaceTempView(\"cases\")\n",
        "\n",
        "df1=spark.sql(\"select c.province,c.city ,sum(c.confirmed) as total_confirmed, sum(r.elementary_school_count) as total_elementary_school_count  \\\n",
        "from cases c right join region r on c.province = r.state and c.city = r.city \\\n",
        "group by c.province,c.city order by  total_elementary_school_count\") \n",
        "df1.show(6)\n"
      ],
      "metadata": {
        "id": "li4ZXuHwaR72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Create Spark UDFs Create function casehighlow(). If case is less than 50 return low else return high.convert into a UDF Function and mention the return type of function."
      ],
      "metadata": {
        "id": "wYtQAzGS77eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "casehighlow = udf(lambda x :  \"High\" if x > 50  else \"Low\",StringType())\n",
        "df2.withColumn(\"new_column\", casehighlow(col('total_confirmed'))).show(10)"
      ],
      "metadata": {
        "id": "yPm7iMAn5FhK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}